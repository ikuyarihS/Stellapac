{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-ItGPKv4QiK"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "--find-links https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "scikit-learn==1.2.2\n",
        "scikit-image==0.19.3\n",
        "scipy==1.10.1\n",
        "rasterio==1.3.7\n",
        "tensorboard==2.12.2\n",
        "tqdm==4.65.0\n",
        "PyYAML==6.0\n",
        "geopandas==0.13.0\n",
        "click==8.1.3\n",
        "torch==2.0.0+cu117\n",
        "torchvision==0.15.1+cu117\n",
        "torchaudio==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification\n",
        "!git clone https://github.com/ClarkCGA/multi-temporal-crop-classification-baseline.git\n",
        "!tar -xvzf /content/multi-temporal-crop-classification/training_chips.tgz\n",
        "!tar -xvzf /content/multi-temporal-crop-classification/validation_chips.tgz"
      ],
      "metadata": {
        "id": "cTWOOgob4TbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/data/\n",
        "!mv /content/training_chips/ /content/data/\n",
        "!mv /content/validation_chips/ /content/data"
      ],
      "metadata": {
        "id": "2J1jWJnf4UXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "# Custom dataset params\n",
        "src_dir: /content/home/data\n",
        "train_dataset_name: training_chips\n",
        "val_dataset_name: validation_chips\n",
        "train_csv_path: /content/github_repo/train_ids.csv\n",
        "val_csv_path: /content/github_repo/test_ids.csv\n",
        "test_csv_path: /content/github_repo/test_ids.csv\n",
        "apply_normalization: true\n",
        "normal_strategy: z_value\n",
        "stat_procedure: gpb\n",
        "global_stats:\n",
        "  min: [124.0, 308.0, 191.0, 598.0, 423.0, 271.0]\n",
        "  max: [1207.0, 1765.0, 2366.0, 4945.0, 4646.0, 3897.0]\n",
        "  mean: [494.905781, 815.239594, 924.335066, 2968.881459, 2634.621962, 1739.579917]\n",
        "  std: [284.925432, 357.84876, 575.566823, 896.601013, 951.900334, 921.407808]\n",
        "transformations:\n",
        "- v_flip\n",
        "- h_flip\n",
        "- d_flip\n",
        "- rotate\n",
        "aug_params:\n",
        "  rotation_degree: [-180, -90, 90, 180]\n",
        "\n",
        "# DataLoader\n",
        "train_BatchSize: 10\n",
        "val_test_BatchSize: 3\n",
        "\n",
        "# Model initialization params\n",
        "n_classes: 14\n",
        "input_channels: 18\n",
        "filter_config: [64, 128, 256, 512, 1024, 1024]\n",
        "use_skipAtt: false\n",
        "train_dropout_rate: 0.15\n",
        "\n",
        "# Model compiler params\n",
        "working_dir: /content/home/workdir\n",
        "out_dir: /content/result\n",
        "class_mapping:\n",
        "  0: Unknown\n",
        "  1: Natural Vegetation\n",
        "  2: Forest\n",
        "  3: Corn\n",
        "  4: Soybeans\n",
        "  5: Wetlands\n",
        "  6: Developed/Barren\n",
        "  7: Open Water\n",
        "  8: Winter Wheat\n",
        "  9: Alfalfa\n",
        "  10: Fallow/Idle Cropland\n",
        "  11: Cotton\n",
        "  12: Sorghum\n",
        "  13: Other\n",
        "gpuDevices:\n",
        "- 0\n",
        "init_type: kaiming\n",
        "params_init: null\n",
        "freeze_params: null\n",
        "\n",
        "# Model fitting\n",
        "epochs: 100\n",
        "optimizer: sam\n",
        "LR: 0.011\n",
        "LR_policy: PolynomialLR\n",
        "criterion:\n",
        "    name: TverskyFocalLoss\n",
        "    weight:\n",
        "    - 0.0182553\n",
        "    - 0.03123664\n",
        "    - 0.02590038\n",
        "    - 0.03026126\n",
        "    - 0.04142966\n",
        "    - 0.04371284\n",
        "    - 0.15352935\n",
        "    - 0.07286951\n",
        "    - 0.10277024\n",
        "    - 0.10736637\n",
        "    - 0.1447082\n",
        "    - 0.17132445\n",
        "    - 0.0566358\n",
        "    ignore_index: 0\n",
        "    gamma: 0.9\n",
        "\n",
        "momentum: 0.95\n",
        "checkpoint_interval: 20\n",
        "resume: false\n",
        "resume_epoch: null\n",
        "lr_prams:\n",
        "  # StepLR & MultiStepLR\n",
        "  step_size: 3\n",
        "  milestones:\n",
        "  - 5\n",
        "  - 10\n",
        "  - 20\n",
        "  - 35\n",
        "  - 50\n",
        "  - 70\n",
        "  - 90\n",
        "  gamma: 0.98\n",
        "  # ReduceLROnPlateau\n",
        "  mode: triangular\n",
        "  factor: 0.8\n",
        "  patience: 3\n",
        "  threshold: 0.0001\n",
        "  threshold_mode: rel\n",
        "  min_lr: 3.0e-06\n",
        "  # PolynomialLR\n",
        "  max_decay_steps: 80\n",
        "  min_learning_rate: 1.0e-04\n",
        "  power: 0.85\n",
        "  # CyclicLR\n",
        "  base_lr: 3.0e-05\n",
        "  max_lr: 0.01\n",
        "  step_size_up: 1100\n",
        "\n",
        "# Accuracy assessment\n",
        "val_metric_fname: validate_metrics_global_z_gpb.csv"
      ],
      "metadata": {
        "id": "QXakX2ia4XOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, copy, time, math, random, numbers, itertools, tqdm, importlib, re\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import rasterio\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from sklearn import metrics\n",
        "from skimage import transform as trans\n",
        "from pathlib import Path\n",
        "from collections.abc import Sequence\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.ndimage import rotate\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "3JuozSBa4p0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_path = os.path.abspath(os.path.join('github_repo/src'))\n",
        "sys.path.insert(0, module_path)\n",
        "from custom_dataset import CropData\n",
        "from models.unet import Unet\n",
        "from model_compiler import ModelCompiler\n",
        "from utils import *\n",
        "from custom_loss_functions import TverskyFocalLoss"
      ],
      "metadata": {
        "id": "18SeDNiB4r3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code cell loads a configuration file (default_config.yaml) using the YAML library and stores the\n",
        "# configuration data in the config dictionary. Then, it processes the global_stats section of the config\n",
        "# dictionary by expanding the lists for each stats based on the number of available time points.\n",
        "# As you can see we decided to generate a single set of normalization statistics and use it to\n",
        "# normalize all the time-points.\n",
        "\n",
        "yaml_config_path = \"/content/config.yaml\"  # replace this path to your own config file.\n",
        "num_time_points = 3  # Change this number accordingly if you use a dataset with a different temporal length.\n",
        "\n",
        "with open(yaml_config_path, 'r') as file:\n",
        "    config = yaml.load(file, Loader=yaml.SafeLoader)\n",
        "\n",
        "# Perform multiplication and concatenation for each key in global_stats\n",
        "for key, value in config['global_stats'].items():\n",
        "    config['global_stats'][key] = value * num_time_points\n",
        "\n",
        "# OPTIONAL\n",
        "# pretty-print the config dictionary\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(config, width=100, compact=True)"
      ],
      "metadata": {
        "id": "Ureoa3DO4vP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CropData(src_dir=config[\"src_dir\"],\n",
        "                         usage=\"train\",\n",
        "                         dataset_name=config[\"train_dataset_name\"],\n",
        "                         csv_path=config[\"train_csv_path\"],\n",
        "                         apply_normalization=config[\"apply_normalization\"],\n",
        "                         normal_strategy=config[\"normal_strategy\"],\n",
        "                         stat_procedure=config[\"stat_procedure\"],\n",
        "                         global_stats=config[\"global_stats\"],\n",
        "                         trans=config[\"transformations\"],\n",
        "                         **config[\"aug_params\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=config[\"train_BatchSize\"],\n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "PI0oJhBU4zFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = CropData(src_dir=config[\"src_dir\"],\n",
        "                       usage=\"validation\",\n",
        "                       dataset_name=config[\"val_dataset_name\"],\n",
        "                       csv_path=config[\"val_csv_path\"],\n",
        "                       apply_normalization=config[\"apply_normalization\"],\n",
        "                       normal_strategy=config[\"normal_strategy\"],\n",
        "                       stat_procedure=config[\"stat_procedure\"],\n",
        "                       global_stats=config[\"global_stats\"],)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=config[\"val_test_BatchSize\"],\n",
        "                        shuffle=False)"
      ],
      "metadata": {
        "id": "cgj-vmCI41xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(n_classes=config[\"n_classes\"],\n",
        "             in_channels=config[\"input_channels\"],\n",
        "             use_skipAtt=config[\"use_skipAtt\"],\n",
        "             filter_config=config[\"filter_config\"],\n",
        "             dropout_rate=config[\"train_dropout_rate\"])"
      ],
      "metadata": {
        "id": "hPjIeH8g46QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model = ModelCompiler(model,\n",
        "                               working_dir=config[\"working_dir\"],\n",
        "                               out_dir=config[\"out_dir\"],\n",
        "                               num_classes=config[\"n_classes\"],\n",
        "                               inch=config[\"input_channels\"],\n",
        "                               class_mapping=config[\"class_mapping\"],\n",
        "                               gpu_devices=config[\"gpuDevices\"],\n",
        "                               model_init_type=config[\"init_type\"],\n",
        "                               params_init=config[\"params_init\"],\n",
        "                               freeze_params=config[\"freeze_params\"])"
      ],
      "metadata": {
        "id": "zwfoggqt478H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_name = config['criterion']['name']\n",
        "weight = config['criterion']['weight']\n",
        "ignore_index = config['criterion']['ignore_index']\n",
        "gamma = config['criterion']['gamma']\n",
        "\n",
        "if criterion_name == 'TverskyFocalLoss':\n",
        "    criterion = TverskyFocalLoss(weight=weight, ignore_index=ignore_index, gamma=gamma)\n",
        "else:\n",
        "    criterion = TverskyFocalLoss(weight=weight, ignore_index=ignore_index)\n",
        "\n",
        "#print(isinstance(criterion, object))\n",
        "\n",
        "compiled_model.fit(train_loader,\n",
        "                   val_loader,\n",
        "                   epochs=config[\"epochs\"],\n",
        "                   optimizer_name=config[\"optimizer\"],\n",
        "                   lr_init=config[\"LR\"],\n",
        "                   lr_policy=config[\"LR_policy\"],\n",
        "                   criterion=criterion,\n",
        "                   momentum=config[\"momentum\"],\n",
        "                   checkpoint_interval=config[\"checkpoint_interval\"],\n",
        "                   resume=config[\"resume\"],\n",
        "                   resume_epoch=config[\"resume_epoch\"],\n",
        "                   **config[\"lr_prams\"])"
      ],
      "metadata": {
        "id": "0YhuDJeK490I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = compiled_model.accuracy_evaluation(val_loader, filename=config[\"val_metric_fname\"])"
      ],
      "metadata": {
        "id": "1Z5oqmU15A4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CropData(src_dir=config[\"src_dir\"],\n",
        "                       usage=\"inference\",\n",
        "                       dataset_name=config[\"val_dataset_name\"],\n",
        "                       csv_path=config[\"val_csv_path\"],\n",
        "                       apply_normalization=config[\"apply_normalization\"],\n",
        "                       normal_strategy=config[\"normal_strategy\"],\n",
        "                       stat_procedure=config[\"stat_procedure\"],\n",
        "                       global_stats=config[\"global_stats\"],)"
      ],
      "metadata": {
        "id": "dUWRlVrc5B3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def meta_handling_collate_fn(batch):\n",
        "    images = []\n",
        "    labels = []\n",
        "    img_ids = []\n",
        "    img_metas = []\n",
        "\n",
        "    # Unpack elements from each sample in the batch\n",
        "    for sample in batch:\n",
        "        images.append(sample[0])\n",
        "        labels.append(sample[1])\n",
        "        img_ids.append(sample[2])\n",
        "        img_metas.append(sample[3])  # append the dict to the list\n",
        "\n",
        "    # Stack images and labels into a single tensor\n",
        "    images = torch.stack(images, dim=0)\n",
        "    labels = torch.stack(labels, dim=0)\n",
        "\n",
        "    return images, labels, img_ids, img_metas\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=config[\"val_test_BatchSize\"],\n",
        "                        shuffle=False,\n",
        "                        collate_fn=meta_handling_collate_fn)"
      ],
      "metadata": {
        "id": "m1jTUiWq5C5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model.inference(test_loader, out_dir=\"/content/result\")"
      ],
      "metadata": {
        "id": "gI5xF4dZ5EkU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}